{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1u50vjq5nbrEK6uoso6sD8ohFZc2AdJJU","timestamp":1663150366128}],"collapsed_sections":[],"authorship_tag":"ABX9TyMdI7eKnmm7jTXT05InxXYE"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"fb0a4d06b88a41919023d3d59bbd014b":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_87fc729dc8c14be48ceed64ef89dea3a","IPY_MODEL_710752824eb340a3a2b7392834ce9008","IPY_MODEL_efc9d5b99777439fb344ae4cb4f85f90"],"layout":"IPY_MODEL_bade6f81a9444f509c6e681e7461745a"}},"87fc729dc8c14be48ceed64ef89dea3a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_940da3088d4841efa3a61254255fbfbe","placeholder":"​","style":"IPY_MODEL_35ba3e2547474148a310bc782576caf5","value":"100%"}},"710752824eb340a3a2b7392834ce9008":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_ac44c8a0da454545be629daaa54f4646","max":1999639040,"min":0,"orientation":"horizontal","style":"IPY_MODEL_60ebe4f2856d4a0bb386278ee958ef52","value":1999639040}},"efc9d5b99777439fb344ae4cb4f85f90":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4a99a9d07db54f3391dadc35f0d04da0","placeholder":"​","style":"IPY_MODEL_b37315a864de488c9180670a3e37fe25","value":" 1999639040/1999639040 [02:20&lt;00:00, 14257963.40it/s]"}},"bade6f81a9444f509c6e681e7461745a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"940da3088d4841efa3a61254255fbfbe":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"35ba3e2547474148a310bc782576caf5":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ac44c8a0da454545be629daaa54f4646":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"60ebe4f2856d4a0bb386278ee958ef52":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"4a99a9d07db54f3391dadc35f0d04da0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b37315a864de488c9180670a3e37fe25":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d356c9ec5a2640288e686584349e90d7":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_8f72c01021734dd5837dc486a358a5c5","IPY_MODEL_fbbd5e378a774e78997ce58568003d5a","IPY_MODEL_3ef4bb1bbd3f4c2589a5572ecf8152ad"],"layout":"IPY_MODEL_cb14e71b1b394836ba9166244feaa9c9"}},"8f72c01021734dd5837dc486a358a5c5":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_761cd9d638334f91a540109aa5a5d2f5","placeholder":"​","style":"IPY_MODEL_472039593c914997aeae18c3c9c0b103","value":"100%"}},"fbbd5e378a774e78997ce58568003d5a":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_f92f4473415c49f181a12d6231244741","max":553433881,"min":0,"orientation":"horizontal","style":"IPY_MODEL_bcd8f021a9894a0bafc3ee8f424800c6","value":553433881}},"3ef4bb1bbd3f4c2589a5572ecf8152ad":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8a78d99edde34327b9409e17a00febd1","placeholder":"​","style":"IPY_MODEL_b2f8565e9ea84c8e87aed071a24a8da3","value":" 528M/528M [00:08&lt;00:00, 86.8MB/s]"}},"cb14e71b1b394836ba9166244feaa9c9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"761cd9d638334f91a540109aa5a5d2f5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"472039593c914997aeae18c3c9c0b103":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f92f4473415c49f181a12d6231244741":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bcd8f021a9894a0bafc3ee8f424800c6":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"8a78d99edde34327b9409e17a00febd1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b2f8565e9ea84c8e87aed071a24a8da3":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}},"gpuClass":"standard","accelerator":"GPU"},"cells":[{"cell_type":"code","source":["import torch\n","from torch import nn\n","from torch.autograd import Function\n","import torchvision\n","from torchvision import transforms\n","from torch.utils import data\n","\n","import os\n","import cv2\n","import numpy as np\n","import pandas as pd\n","from numpy import random\n","import matplotlib.pyplot as plt\n","import pdb\n","import tarfile\n","import xml.etree.ElementTree as ET"],"metadata":{"id":"s81C24T5PSmu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def point_form(boxes):\n","    \"\"\" Convert prior_boxes to (xmin, ymin, xmax, ymax)\n","    representation for comparison to point form ground truth data.\n","    Args:\n","        boxes: (tensor) center-size default boxes from priorbox layers.\n","    Return:\n","        boxes: (tensor) Converted xmin, ymin, xmax, ymax form of boxes.\n","    \"\"\"\n","    return torch.cat((boxes[:, :2] - boxes[:, 2:]/2,     # xmin, ymin\n","                     boxes[:, :2] + boxes[:, 2:]/2), 1)  # xmax, ymax\n","\n","\n","def center_size(boxes):\n","    \"\"\" Convert prior_boxes to (cx, cy, w, h)\n","    representation for comparison to center-size form ground truth data.\n","    Args:\n","        boxes: (tensor) point_form boxes\n","    Return:\n","        boxes: (tensor) Converted xmin, ymin, xmax, ymax form of boxes.\n","    \"\"\"\n","    return torch.cat((boxes[:, 2:] + boxes[:, :2])/2,  # cx, cy\n","                     boxes[:, 2:] - boxes[:, :2], 1)  # w, h\n","\n","\n","def intersect(box_a, box_b):\n","    \"\"\" We resize both tensors to [A,B,2] without new malloc:\n","    [A,2] -> [A,1,2] -> [A,B,2]\n","    [B,2] -> [1,B,2] -> [A,B,2]\n","    Then we compute the area of intersect between box_a and box_b.\n","    Args:\n","      box_a: (tensor) bounding boxes, Shape: [A,4].\n","      box_b: (tensor) bounding boxes, Shape: [B,4].\n","    Return:\n","      (tensor) intersection area, Shape: [A,B].\n","    \"\"\"\n","    A = box_a.size(0)\n","    B = box_b.size(0)\n","    max_xy = torch.min(box_a[:, 2:].unsqueeze(1).expand(A, B, 2),\n","                       box_b[:, 2:].unsqueeze(0).expand(A, B, 2))\n","    min_xy = torch.max(box_a[:, :2].unsqueeze(1).expand(A, B, 2),\n","                       box_b[:, :2].unsqueeze(0).expand(A, B, 2))\n","    inter = torch.clamp((max_xy - min_xy), min=0)\n","    return inter[:, :, 0] * inter[:, :, 1]\n","\n","\n","def jaccard(box_a, box_b):\n","    \"\"\"Compute the jaccard overlap of two sets of boxes.  The jaccard overlap\n","    is simply the intersection over union of two boxes.  Here we operate on\n","    ground truth boxes and default boxes.\n","    E.g.:\n","        A ∩ B / A ∪ B = A ∩ B / (area(A) + area(B) - A ∩ B)\n","    Args:\n","        box_a: (tensor) Ground truth bounding boxes, Shape: [num_objects,4]\n","        box_b: (tensor) Prior boxes from priorbox layers, Shape: [num_priors,4]\n","    Return:\n","        jaccard overlap: (tensor) Shape: [box_a.size(0), box_b.size(0)]\n","    \"\"\"\n","    inter = intersect(box_a, box_b)\n","    area_a = ((box_a[:, 2]-box_a[:, 0]) *\n","              (box_a[:, 3]-box_a[:, 1])).unsqueeze(1).expand_as(inter)  # [A,B]\n","    area_b = ((box_b[:, 2]-box_b[:, 0]) *\n","              (box_b[:, 3]-box_b[:, 1])).unsqueeze(0).expand_as(inter)  # [A,B]\n","    union = area_a + area_b - inter\n","    return inter / union  # [A,B]\n","\n","\n","def match(threshold, truths, priors, variances, labels, loc_t, conf_t, idx):\n","    \"\"\"Match each prior box with the ground truth box of the highest jaccard\n","    overlap, encode the bounding boxes, then return the matched indices\n","    corresponding to both confidence and location preds.\n","    Args:\n","        threshold: (float) The overlap threshold used when mathing boxes.\n","        truths: (tensor) Ground truth boxes, Shape: [num_obj, num_priors].\n","        priors: (tensor) Prior boxes from priorbox layers, Shape: [n_priors,4].\n","        variances: (tensor) Variances corresponding to each prior coord,\n","            Shape: [num_priors, 4].\n","        labels: (tensor) All the class labels for the image, Shape: [num_obj].\n","        loc_t: (tensor) Tensor to be filled w/ endcoded location targets.\n","        conf_t: (tensor) Tensor to be filled w/ matched indices for conf preds.\n","        idx: (int) current batch index\n","    Return:\n","        The matched indices corresponding to 1)location and 2)confidence preds.\n","    \"\"\"\n","    # jaccard index\n","    overlaps = jaccard(\n","        truths,\n","        point_form(priors)\n","    )\n","    # (Bipartite Matching)\n","    # [1,num_objects] best prior for each ground truth\n","    best_prior_overlap, best_prior_idx = overlaps.max(1, keepdim=True)\n","    # [1,num_priors] best ground truth for each prior\n","    best_truth_overlap, best_truth_idx = overlaps.max(0, keepdim=True)\n","    best_truth_idx.squeeze_(0)\n","    best_truth_overlap.squeeze_(0)\n","    best_prior_idx.squeeze_(1)\n","    best_prior_overlap.squeeze_(1)\n","    best_truth_overlap.index_fill_(0, best_prior_idx, 2)  # ensure best prior\n","    # TODO refactor: index  best_prior_idx with long tensor\n","    # ensure every gt matches with its prior of max overlap\n","    for j in range(best_prior_idx.size(0)):\n","        best_truth_idx[best_prior_idx[j]] = j\n","    matches = truths[best_truth_idx]          # Shape: [num_priors,4]\n","    conf = labels[best_truth_idx] + 1         # Shape: [num_priors]\n","    conf[best_truth_overlap < threshold] = 0  # label as background\n","    loc = encode(matches, priors, variances)\n","    loc_t[idx] = loc    # [num_priors,4] encoded offsets to learn\n","    conf_t[idx] = conf  # [num_priors] top class label for each prior\n","\n","\n","def encode(matched, priors, variances):\n","    \"\"\"Encode the variances from the priorbox layers into the ground truth boxes\n","    we have matched (based on jaccard overlap) with the prior boxes.\n","    Args:\n","        matched: (tensor) Coords of ground truth for each prior in point-form\n","            Shape: [num_priors, 4].\n","        priors: (tensor) Prior boxes in center-offset form\n","            Shape: [num_priors,4].\n","        variances: (list[float]) Variances of priorboxes\n","    Return:\n","        encoded boxes (tensor), Shape: [num_priors, 4]\n","    \"\"\"\n","\n","    # dist b/t match center and prior's center\n","    g_cxcy = (matched[:, :2] + matched[:, 2:])/2 - priors[:, :2]\n","    # encode variance\n","    g_cxcy /= (variances[0] * priors[:, 2:])\n","    # match wh / prior wh\n","    g_wh = (matched[:, 2:] - matched[:, :2]) / priors[:, 2:]\n","    g_wh = torch.log(g_wh) / variances[1]\n","    # return target for smooth_l1_loss\n","    return torch.cat([g_cxcy, g_wh], 1)  # [num_priors,4]\n","\n","\n","# Adapted from https://github.com/Hakuyume/chainer-ssd\n","def decode(loc, priors, variances):\n","    \"\"\"Decode locations from predictions using priors to undo\n","    the encoding we did for offset regression at train time.\n","    Args:\n","        loc (tensor): location predictions for loc layers,\n","            Shape: [num_priors,4]\n","        priors (tensor): Prior boxes in center-offset form.\n","            Shape: [num_priors,4].\n","        variances: (list[float]) Variances of priorboxes\n","    Return:\n","        decoded bounding box predictions\n","    \"\"\"\n","\n","    boxes = torch.cat((\n","        priors[:, :2] + loc[:, :2] * variances[0] * priors[:, 2:],\n","        priors[:, 2:] * torch.exp(loc[:, 2:] * variances[1])), 1)\n","    boxes[:, :2] -= boxes[:, 2:] / 2\n","    boxes[:, 2:] += boxes[:, :2]\n","    return boxes\n","\n","\n","def log_sum_exp(x):\n","    \"\"\"Utility function for computing log_sum_exp while determining\n","    This will be used to determine unaveraged confidence loss across\n","    all examples in a batch.\n","    Args:\n","        x (Variable(tensor)): conf_preds from conf layers\n","    \"\"\"\n","    x_max = x.data.max()\n","    return torch.log(torch.sum(torch.exp(x-x_max), 1, keepdim=True)) + x_max\n","\n","\n","# Original author: Francisco Massa:\n","# https://github.com/fmassa/object-detection.torch\n","# Ported to PyTorch by Max deGroot (02/01/2017)\n","def nms(boxes, scores, overlap=0.5, top_k=200):\n","    \"\"\"Apply non-maximum suppression at test time to avoid detecting too many\n","    overlapping bounding boxes for a given object.\n","    Args:\n","        boxes: (tensor) The location preds for the img, Shape: [num_priors,4].\n","        scores: (tensor) The class predscores for the img, Shape:[num_priors].\n","        overlap: (float) The overlap thresh for suppressing unnecessary boxes.\n","        top_k: (int) The Maximum number of box preds to consider.\n","    Return:\n","        The indices of the kept boxes with respect to num_priors.\n","    \"\"\"\n","\n","    keep = scores.new(scores.size(0)).zero_().long()\n","    if boxes.numel() == 0:\n","        return keep\n","    x1 = boxes[:, 0]\n","    y1 = boxes[:, 1]\n","    x2 = boxes[:, 2]\n","    y2 = boxes[:, 3]\n","    area = torch.mul(x2 - x1, y2 - y1)\n","    v, idx = scores.sort(0)  # sort in ascending order\n","    # I = I[v >= 0.01]\n","    idx = idx[-top_k:]  # indices of the top-k largest vals\n","    xx1 = boxes.new()\n","    yy1 = boxes.new()\n","    xx2 = boxes.new()\n","    yy2 = boxes.new()\n","    w = boxes.new()\n","    h = boxes.new()\n","\n","    # keep = torch.Tensor()\n","    count = 0\n","    while idx.numel() > 0:\n","        i = idx[-1]  # index of current largest val\n","        # keep.append(i)\n","        keep[count] = i\n","        count += 1\n","        if idx.size(0) == 1:\n","            break\n","        idx = idx[:-1]  # remove kept element from view\n","        # load bboxes of next highest vals\n","        torch.index_select(x1, 0, idx, out=xx1)\n","        torch.index_select(y1, 0, idx, out=yy1)\n","        torch.index_select(x2, 0, idx, out=xx2)\n","        torch.index_select(y2, 0, idx, out=yy2)\n","        # store element-wise max with next highest score\n","        xx1 = torch.clamp(xx1, min=x1[i])\n","        yy1 = torch.clamp(yy1, min=y1[i])\n","        xx2 = torch.clamp(xx2, max=x2[i])\n","        yy2 = torch.clamp(yy2, max=y2[i])\n","        w.resize_as_(xx2)\n","        h.resize_as_(yy2)\n","        w = xx2 - xx1\n","        h = yy2 - yy1\n","        # check sizes of xx1 and xx2.. after each iteration\n","        w = torch.clamp(w, min=0.0)\n","        h = torch.clamp(h, min=0.0)\n","        inter = w*h\n","        # IoU = i / (area(a) + area(b) - i)\n","        rem_areas = torch.index_select(area, 0, idx)  # load remaining areas)\n","        union = (rem_areas - inter) + area[i]\n","        IoU = inter/union  # store result in iou\n","        # keep only elements with an IoU <= overlap\n","        idx = idx[IoU.le(overlap)]\n","    return keep, count"],"metadata":{"id":"wfDivALsv25i"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","\n","def intersect_numpy(box_a, box_b):\n","    max_xy = np.minimum(box_a[:, 2:], box_b[2:])\n","    min_xy = np.maximum(box_a[:, :2], box_b[:2])\n","    inter = np.clip((max_xy - min_xy), a_min=0, a_max=np.inf)\n","    return inter[:, 0] * inter[:, 1]\n","\n","\n","def jaccard_numpy(box_a, box_b):\n","    \"\"\"Compute the jaccard overlap of two sets of boxes.  The jaccard overlap\n","    is simply the intersection over union of two boxes.\n","    E.g.:\n","        A ∩ B / A ∪ B = A ∩ B / (area(A) + area(B) - A ∩ B)\n","    Args:\n","        box_a: Multiple bounding boxes, Shape: [num_boxes,4]\n","        box_b: Single bounding box, Shape: [4]\n","    Return:\n","        jaccard overlap: Shape: [box_a.shape[0], box_a.shape[1]]\n","    \"\"\"\n","    inter = intersect_numpy(box_a, box_b)\n","    area_a = ((box_a[:, 2]-box_a[:, 0]) *\n","              (box_a[:, 3]-box_a[:, 1]))  # [A,B]\n","    area_b = ((box_b[2]-box_b[0]) *\n","              (box_b[3]-box_b[1]))  # [A,B]\n","    union = area_a + area_b - inter\n","    return inter / union  # [A,B]\n","\n","\n","class Compose(object):\n","    \"\"\"Composes several augmentations together.\n","    Args:\n","        transforms (List[Transform]): list of transforms to compose.\n","    Example:\n","        >>> augmentations.Compose([\n","        >>>     transforms.CenterCrop(10),\n","        >>>     transforms.ToTensor(),\n","        >>> ])\n","    \"\"\"\n","\n","    def __init__(self, transforms):\n","        self.transforms = transforms\n","\n","    def __call__(self, img, boxes=None, labels=None):\n","        for t in self.transforms:\n","            img, boxes, labels = t(img, boxes, labels)\n","        return img, boxes, labels\n","\n","\n","class Lambda(object):\n","    \"\"\"Applies a lambda as a transform.\"\"\"\n","\n","    def __init__(self, lambd):\n","        assert isinstance(lambd, types.LambdaType)\n","        self.lambd = lambd\n","\n","    def __call__(self, img, boxes=None, labels=None):\n","        return self.lambd(img, boxes, labels)\n","\n","\n","class ConvertFromInts(object):\n","    def __call__(self, image, boxes=None, labels=None):\n","        return image.astype(np.float32), boxes, labels\n","\n","\n","class SubtractMeans(object):\n","    def __init__(self, mean):\n","        self.mean = np.array(mean, dtype=np.float32)\n","\n","    def __call__(self, image, boxes=None, labels=None):\n","        image = image.astype(np.float32)\n","        image -= self.mean\n","        return image.astype(np.float32), boxes, labels\n","\n","\n","class ToAbsoluteCoords(object):\n","    def __call__(self, image, boxes=None, labels=None):\n","        height, width, channels = image.shape\n","        boxes[:, 0] *= width\n","        boxes[:, 2] *= width\n","        boxes[:, 1] *= height\n","        boxes[:, 3] *= height\n","\n","        return image, boxes, labels\n","\n","\n","class ToPercentCoords(object):\n","    def __call__(self, image, boxes=None, labels=None):\n","        height, width, channels = image.shape\n","        boxes[:, 0] /= width\n","        boxes[:, 2] /= width\n","        boxes[:, 1] /= height\n","        boxes[:, 3] /= height\n","\n","        return image, boxes, labels\n","\n","\n","class Resize(object):\n","    def __init__(self, size=300):\n","        self.size = size\n","\n","    def __call__(self, image, boxes=None, labels=None):\n","        image = cv2.resize(image, (self.size,\n","                                 self.size))\n","        return image, boxes, labels\n","\n","\n","class RandomSaturation(object):\n","    def __init__(self, lower=0.5, upper=1.5):\n","        self.lower = lower\n","        self.upper = upper\n","        assert self.upper >= self.lower, \"contrast upper must be >= lower.\"\n","        assert self.lower >= 0, \"contrast lower must be non-negative.\"\n","\n","    def __call__(self, image, boxes=None, labels=None):\n","        if random.randint(2):\n","            image[:, :, 1] *= random.uniform(self.lower, self.upper)\n","\n","        return image, boxes, labels\n","\n","\n","class RandomHue(object):\n","    def __init__(self, delta=18.0):\n","        assert delta >= 0.0 and delta <= 360.0\n","        self.delta = delta\n","\n","    def __call__(self, image, boxes=None, labels=None):\n","        if random.randint(2):\n","            image[:, :, 0] += random.uniform(-self.delta, self.delta)\n","            image[:, :, 0][image[:, :, 0] > 360.0] -= 360.0\n","            image[:, :, 0][image[:, :, 0] < 0.0] += 360.0\n","        return image, boxes, labels\n","\n","\n","class RandomLightingNoise(object):\n","    def __init__(self):\n","        self.perms = ((0, 1, 2), (0, 2, 1),\n","                      (1, 0, 2), (1, 2, 0),\n","                      (2, 0, 1), (2, 1, 0))\n","\n","    def __call__(self, image, boxes=None, labels=None):\n","        if random.randint(2):\n","            swap = self.perms[random.randint(len(self.perms))]\n","            shuffle = SwapChannels(swap)  # shuffle channels\n","            image = shuffle(image)\n","        return image, boxes, labels\n","\n","\n","class ConvertColor(object):\n","    def __init__(self, current='BGR', transform='HSV'):\n","        self.transform = transform\n","        self.current = current\n","\n","    def __call__(self, image, boxes=None, labels=None):\n","        if self.current == 'BGR' and self.transform == 'HSV':\n","            image = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n","        elif self.current == 'HSV' and self.transform == 'BGR':\n","            image = cv2.cvtColor(image, cv2.COLOR_HSV2BGR)\n","        else:\n","            raise NotImplementedError\n","        return image, boxes, labels\n","\n","\n","class RandomContrast(object):\n","    def __init__(self, lower=0.5, upper=1.5):\n","        self.lower = lower\n","        self.upper = upper\n","        assert self.upper >= self.lower, \"contrast upper must be >= lower.\"\n","        assert self.lower >= 0, \"contrast lower must be non-negative.\"\n","\n","    # expects float image\n","    def __call__(self, image, boxes=None, labels=None):\n","        if random.randint(2):\n","            alpha = random.uniform(self.lower, self.upper)\n","            image *= alpha\n","        return image, boxes, labels\n","\n","\n","class RandomBrightness(object):\n","    def __init__(self, delta=32):\n","        assert delta >= 0.0\n","        assert delta <= 255.0\n","        self.delta = delta\n","\n","    def __call__(self, image, boxes=None, labels=None):\n","        if random.randint(2):\n","            delta = random.uniform(-self.delta, self.delta)\n","            image += delta\n","        return image, boxes, labels\n","\n","\n","class ToCV2Image(object):\n","    def __call__(self, tensor, boxes=None, labels=None):\n","        return tensor.cpu().numpy().astype(np.float32).transpose((1, 2, 0)), boxes, labels\n","\n","\n","class ToTensor(object):\n","    def __call__(self, cvimage, boxes=None, labels=None):\n","        return torch.from_numpy(cvimage.astype(np.float32)).permute(2, 0, 1), boxes, labels\n","\n","\n","class RandomSampleCrop(object):\n","    \"\"\"Crop\n","    Arguments:\n","        img (Image): the image being input during training\n","        boxes (Tensor): the original bounding boxes in pt form\n","        labels (Tensor): the class labels for each bbox\n","        mode (float tuple): the min and max jaccard overlaps\n","    Return:\n","        (img, boxes, classes)\n","            img (Image): the cropped image\n","            boxes (Tensor): the adjusted bounding boxes in pt form\n","            labels (Tensor): the class labels for each bbox\n","    \"\"\"\n","    def __init__(self):\n","        self.sample_options = (\n","            # using entire original input image\n","            None,\n","            # sample a patch s.t. MIN jaccard w/ obj in .1,.3,.4,.7,.9\n","            (0.1, None),\n","            (0.3, None),\n","            (0.7, None),\n","            (0.9, None),\n","            # randomly sample a patch\n","            (None, None),\n","        )\n","\n","    def __call__(self, image, boxes=None, labels=None):\n","        height, width, _ = image.shape\n","        while True:\n","            # randomly choose a mode\n","            mode = random.choice(self.sample_options)\n","            if mode is None:\n","                return image, boxes, labels\n","\n","            min_iou, max_iou = mode\n","            if min_iou is None:\n","                min_iou = float('-inf')\n","            if max_iou is None:\n","                max_iou = float('inf')\n","\n","            # max trails (50)\n","            for _ in range(50):\n","                current_image = image\n","\n","                w = random.uniform(0.3 * width, width)\n","                h = random.uniform(0.3 * height, height)\n","\n","                # aspect ratio constraint b/t .5 & 2\n","                if h / w < 0.5 or h / w > 2:\n","                    continue\n","\n","                left = random.uniform(width - w)\n","                top = random.uniform(height - h)\n","\n","                # convert to integer rect x1,y1,x2,y2\n","                rect = np.array([int(left), int(top), int(left+w), int(top+h)])\n","\n","                # calculate IoU (jaccard overlap) b/t the cropped and gt boxes\n","                overlap = jaccard_numpy(boxes, rect)\n","\n","                # is min and max overlap constraint satisfied? if not try again\n","                if overlap.min() < min_iou and max_iou < overlap.max():\n","                    continue\n","\n","                # cut the crop from the image\n","                current_image = current_image[rect[1]:rect[3], rect[0]:rect[2],\n","                                              :]\n","\n","                # keep overlap with gt box IF center in sampled patch\n","                centers = (boxes[:, :2] + boxes[:, 2:]) / 2.0\n","\n","                # mask in all gt boxes that above and to the left of centers\n","                m1 = (rect[0] < centers[:, 0]) * (rect[1] < centers[:, 1])\n","\n","                # mask in all gt boxes that under and to the right of centers\n","                m2 = (rect[2] > centers[:, 0]) * (rect[3] > centers[:, 1])\n","\n","                # mask in that both m1 and m2 are true\n","                mask = m1 * m2\n","\n","                # have any valid boxes? try again if not\n","                if not mask.any():\n","                    continue\n","\n","                # take only matching gt boxes\n","                current_boxes = boxes[mask, :].copy()\n","\n","                # take only matching gt labels\n","                current_labels = labels[mask]\n","\n","                # should we use the box left and top corner or the crop's\n","                current_boxes[:, :2] = np.maximum(current_boxes[:, :2],\n","                                                  rect[:2])\n","                # adjust to crop (by substracting crop's left,top)\n","                current_boxes[:, :2] -= rect[:2]\n","\n","                current_boxes[:, 2:] = np.minimum(current_boxes[:, 2:],\n","                                                  rect[2:])\n","                # adjust to crop (by substracting crop's left,top)\n","                current_boxes[:, 2:] -= rect[:2]\n","\n","                return current_image, current_boxes, current_labels\n","\n","\n","class Expand(object):\n","    def __init__(self, mean):\n","        self.mean = mean\n","\n","    def __call__(self, image, boxes, labels):\n","        if random.randint(2):\n","            return image, boxes, labels\n","\n","        height, width, depth = image.shape\n","        ratio = random.uniform(1, 4)\n","        left = random.uniform(0, width*ratio - width)\n","        top = random.uniform(0, height*ratio - height)\n","\n","        expand_image = np.zeros(\n","            (int(height*ratio), int(width*ratio), depth),\n","            dtype=image.dtype)\n","        expand_image[:, :, :] = self.mean\n","        expand_image[int(top):int(top + height),\n","                     int(left):int(left + width)] = image\n","        image = expand_image\n","\n","        boxes = boxes.copy()\n","        boxes[:, :2] += (int(left), int(top))\n","        boxes[:, 2:] += (int(left), int(top))\n","\n","        return image, boxes, labels\n","\n","\n","class RandomMirror(object):\n","    def __call__(self, image, boxes, classes):\n","        _, width, _ = image.shape\n","        if random.randint(2):\n","            image = image[:, ::-1]\n","            boxes = boxes.copy()\n","            boxes[:, 0::2] = width - boxes[:, 2::-2]\n","        return image, boxes, classes\n","\n","\n","class SwapChannels(object):\n","    \"\"\"Transforms a tensorized image by swapping the channels in the order\n","     specified in the swap tuple.\n","    Args:\n","        swaps (int triple): final order of channels\n","            eg: (2, 1, 0)\n","    \"\"\"\n","\n","    def __init__(self, swaps):\n","        self.swaps = swaps\n","\n","    def __call__(self, image):\n","        \"\"\"\n","        Args:\n","            image (Tensor): image tensor to be transformed\n","        Return:\n","            a tensor with channels swapped according to swap\n","        \"\"\"\n","        # if torch.is_tensor(image):\n","        #     image = image.data.cpu().numpy()\n","        # else:\n","        #     image = np.array(image)\n","        image = image[:, :, self.swaps]\n","        return image\n","\n","\n","class PhotometricDistort(object):\n","    def __init__(self):\n","        self.pd = [\n","            RandomContrast(),\n","            ConvertColor(transform='HSV'),\n","            RandomSaturation(),\n","            RandomHue(),\n","            ConvertColor(current='HSV', transform='BGR'),\n","            RandomContrast()\n","        ]\n","        self.rand_brightness = RandomBrightness()\n","        self.rand_light_noise = RandomLightingNoise()\n","\n","    def __call__(self, image, boxes, labels):\n","        im = image.copy()\n","        im, boxes, labels = self.rand_brightness(im, boxes, labels)\n","        if random.randint(2):\n","            distort = Compose(self.pd[:-1])\n","        else:\n","            distort = Compose(self.pd[1:])\n","        im, boxes, labels = distort(im, boxes, labels)\n","        return self.rand_light_noise(im, boxes, labels)\n","\n","\n","class SSDAugmentation(object):\n","    def __init__(self, size=300, mean=(104, 117, 123)):\n","        self.mean = mean\n","        self.size = size\n","        self.augment = Compose([\n","            ConvertFromInts(),\n","            ToAbsoluteCoords(),\n","            PhotometricDistort(),\n","            Expand(self.mean),\n","            RandomSampleCrop(),\n","            RandomMirror(),\n","            ToPercentCoords(),\n","            Resize(self.size),\n","            SubtractMeans(self.mean)\n","        ])\n","\n","    def __call__(self, img, boxes, labels):\n","        return self.augment(img, boxes, labels)"],"metadata":{"id":"OmFIuYrOLAQ-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"ex2VCYRo7Vlz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["torchvision.datasets.utils.download_url('http://host.robots.ox.ac.uk/pascal/VOC/voc2012/VOCtrainval_11-May-2012.tar',os.getcwd())\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":67,"referenced_widgets":["fb0a4d06b88a41919023d3d59bbd014b","87fc729dc8c14be48ceed64ef89dea3a","710752824eb340a3a2b7392834ce9008","efc9d5b99777439fb344ae4cb4f85f90","bade6f81a9444f509c6e681e7461745a","940da3088d4841efa3a61254255fbfbe","35ba3e2547474148a310bc782576caf5","ac44c8a0da454545be629daaa54f4646","60ebe4f2856d4a0bb386278ee958ef52","4a99a9d07db54f3391dadc35f0d04da0","b37315a864de488c9180670a3e37fe25"]},"id":"XyNHmfoR-PK9","executionInfo":{"status":"ok","timestamp":1663056702430,"user_tz":-540,"elapsed":141971,"user":{"displayName":"bong bong","userId":"17774416216702601338"}},"outputId":"7adee2dc-8aa1-41ad-c02c-39ccce3bb8c4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading http://host.robots.ox.ac.uk/pascal/VOC/voc2012/VOCtrainval_11-May-2012.tar to /content/VOCtrainval_11-May-2012.tar\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/1999639040 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fb0a4d06b88a41919023d3d59bbd014b"}},"metadata":{}}]},{"cell_type":"code","source":["with tarfile.open('/content/VOCtrainval_11-May-2012.tar') as tar:\n","    tar.extractall(path = '.')"],"metadata":{"id":"FOCoW3U6-ewR"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DOwtmO2x-BWO"},"outputs":[],"source":["def make_datapath_list(rootpath):\n","    imgpath_template = os.path.join(rootpath, 'JPEGImages','%s.jpg')\n","    annopath_template = os.path.join(rootpath, 'Annotations','%s.xml')\n","\n","    train_id_names = os.path.join(rootpath + 'ImageSets/Main/train.txt')\n","    val_id_names = os.path.join(rootpath + 'ImageSets/Main/val.txt')\n","\n","    train_img_list = []\n","    train_anno_list = []\n","    \n","    for line in open(train_id_names):\n","        file_id = line.strip()\n","        img_path = (imgpath_template % file_id)\n","        anno_path = (annopath_template % file_id)\n","        train_img_list.append(img_path)\n","        train_anno_list.append(anno_path)\n","    \n","    val_img_list = []\n","    val_anno_list = []\n","\n","    for line in open(val_id_names):\n","        file_id = line.strip()\n","        img_path = (imgpath_template % file_id)\n","        anno_path = (annopath_template % file_id)\n","        val_img_list.append(img_path)\n","        val_anno_list.append(anno_path)\n","\n","    return train_img_list, train_anno_list, val_img_list, val_anno_list\n"]},{"cell_type":"code","source":["class Anno_xml2list(object):\n","    def __init__(self,classes):\n","        self.classes = classes\n","    \n","    def __call__(self, xml_path, width, height):\n","        ret = []\n","        xml = ET.parse(xml_path).getroot()\n","        for obj in xml.iter('object'):\n","            difficult = int(obj.find('difficult').text)\n","            if difficult ==1:\n","                continue\n","            \n","            bndbox = []\n","            name = obj.find('name').text.lower().strip()\n","            bbox = obj.find('bndbox')\n","\n","            pts = ['xmin', 'ymin', 'xmax', 'ymax']\n","            for pt in pts:\n","                cur_pixel = int(bbox.find(pt).text) -1\n","                if pt == 'xmin' or pt == 'xmax' :\n","                    cur_pixel /= width\n","                else:\n","                    cur_pixel /= height\n","                bndbox.append(cur_pixel)\n","            \n","        label_idx = self.classes.index(name)\n","        bndbox.append(label_idx)\n","        ret +=[bndbox]\n","\n","        return np.array(ret)"],"metadata":{"id":"9YJbt_o98nbY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class DataTransform():\n","    def __init__(self, input_size, color_mean):\n","        self.data_transform = {\n","            'train': Compose([\n","                ConvertFromInts(),\n","                ToAbsoluteCoords(),\n","                PhotometricDistort(),\n","                Expand(color_mean),\n","                RandomSampleCrop(),\n","                RandomMirror(),\n","                ToPercentCoords(),\n","                Resize(input_size),\n","                SubtractMeans(color_mean)\n","            ]),\n","            'val': Compose([\n","                ConvertFromInts(),\n","                Resize(input_size),\n","                SubtractMeans(color_mean)\n","            ])\n","        }\n","    \n","    def __call__(self, img, phase, boxes, labels):\n","        return self.data_transform[phase](img,boxes,labels)"],"metadata":{"id":"9P5v91sxckJV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","\n","# image_file_path = train_img_list[0]\n","# img = cv2.imread(image_file_path)\n","# h, w, c = img.shape\n","# transform_anno = Anno_xml2list(voc_classes)\n","# anno_list = transform_anno(train_anno_list[0],w,h)\n","# plt.imshow(cv2.cvtColor(img,cv2.COLOR_BGR2RGB))\n","\n","# plt.show()\n","\n","\n","# transform = DataTransform(input_size, color_mean)\n","\n","# phase = 'train'\n","# img_transformed, boxes, labels= transform(img, phase, anno_list[:,:4], anno_list[:,4])\n","# plt.imshow(cv2.cvtColor(img_transformed, cv2.COLOR_BGR2RGB))\n","# plt.show()\n","\n","# phase = 'val'\n","# img_transformed, boxes, labels= transform(img, phase, anno_list[:,:4], anno_list[:,4])\n","# plt.imshow(cv2.cvtColor(img_transformed, cv2.COLOR_BGR2RGB))\n","# plt.show()\n","\n"],"metadata":{"id":"pXB_R96WdX9w"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class VOCDataset(data.Dataset):\n","    def __init__(self,img_list, anno_list, phase, transform, transform_anno):\n","        self.img_list = img_list\n","        self.anno_list = anno_list\n","        self.phase = phase\n","        self.transform = transform\n","        self.transform_anno = transform_anno \n","    \n","    def __len__(self):\n","        return len(self.img_list)\n","\n","    def __getitem__(self,index):\n","        im, gt, h, w = self.pull_item(index)\n","\n","        return im, gt\n","\n","    def pull_item(self, index):\n","        image_file_path = self.img_list[index]\n","        img = cv2.imread(image_file_path)\n","        h, w, c = img.shape\n","        anno_file_path = self.anno_list[index]\n","        anno_list = self.transform_anno(anno_file_path, w,h)\n","\n","        img, boxes, labels = self.transform(img, self.phase, anno_list[:,:4], anno_list[:,4])\n","        img = torch.from_numpy(img[:,:,(2,1,0)]).permute(2,0,1)\n","        gt = np.hstack((boxes, np.expand_dims(labels, axis= 1)))\n","\n","        return img, gt, h, w\n"],"metadata":{"id":"7Zs4RV7oW7k0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","\n","# val_dataset.__getitem__(1)"],"metadata":{"id":"rm2aSEvrdGBn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def od_collate_fn(batch):\n","    targets = []\n","    imgs = []\n","    for sample in batch:\n","        imgs.append(sample[0])\n","        targets.append(torch.FloatTensor(sample[1]))\n","    \n","    imgs = torch.stack(imgs, dim=0)\n","    return imgs, targets"],"metadata":{"id":"3-UzPu4NebmE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","\n","# batch_iterator = iter(dataloaders_dict['val'])\n","# img, tar = next(batch_iterator)\n","# print(img.size(), len(tar), tar[1])"],"metadata":{"id":"J_hB9kRCaswa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def make_vgg():\n","    layers = []\n","    in_channels = 3\n","\n","    cfg = [64,64,'M',128,128,'M',256,256,256,'MC',512,512,512,'M',512,512,512]\n","\n","    for v in cfg:\n","        if v == 'M':\n","            layers += [nn.MaxPool2d(kernel_size = 2, stride = 2)]\n","        elif v == 'MC':\n","            layers += [nn.MaxPool2d(kernel_size = 2, stride = 2, ceil_mode = True)]\n","        else:\n","            conv2d = nn.Conv2d(in_channels, v, kernel_size = 3, padding = 1)\n","            layers += [conv2d, nn.ReLU(inplace = True)]\n","            in_channels = v\n","    \n","    pool5 = nn.MaxPool2d(kernel_size = 3, stride = 1, padding = 1)\n","    conv6  = nn.Conv2d(512, 1024, kernel_size = 3, padding = 6, dilation = 6)\n","    conv7  = nn.Conv2d(1024, 1024, kernel_size = 1)\n","    layers += [pool5, conv6, nn.ReLU(inplace = True), conv7, nn.ReLU(inplace = True)]\n","    return nn.ModuleList(layers)\n","\n","vgg_test = make_vgg()\n","for n, p in vgg_test.state_dict().items():\n","    print(n) \n","# print(vgg_test) \n","# state_dict = vgg_test.state_dict()\n","# state_dict['31.weight']"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NOSrsQzpdqyT","executionInfo":{"status":"ok","timestamp":1663056714866,"user_tz":-540,"elapsed":32,"user":{"displayName":"bong bong","userId":"17774416216702601338"}},"outputId":"e43fcc57-ce30-46f0-a44a-017c66b2618a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["0.weight\n","0.bias\n","2.weight\n","2.bias\n","5.weight\n","5.bias\n","7.weight\n","7.bias\n","10.weight\n","10.bias\n","12.weight\n","12.bias\n","14.weight\n","14.bias\n","17.weight\n","17.bias\n","19.weight\n","19.bias\n","21.weight\n","21.bias\n","24.weight\n","24.bias\n","26.weight\n","26.bias\n","28.weight\n","28.bias\n","31.weight\n","31.bias\n","33.weight\n","33.bias\n"]}]},{"cell_type":"code","source":["def make_extras():\n","    layers = []\n","    in_channels = 1024\n","\n","    cfg = [256,512,128,256,128,256,128,256]\n","\n","    layers +=[nn.Conv2d(in_channels, cfg[0], kernel_size = (1))]\n","    layers +=[nn.Conv2d(cfg[0], cfg[1], kernel_size = (3), stride = 2, padding = 1)]\n","    layers +=[nn.Conv2d(cfg[1], cfg[2], kernel_size = (1))]\n","    layers +=[nn.Conv2d(cfg[2], cfg[3], kernel_size = (3), stride = 2, padding = 1)]\n","    layers +=[nn.Conv2d(cfg[3], cfg[4], kernel_size = (1))]\n","    layers +=[nn.Conv2d(cfg[4], cfg[5], kernel_size = (3))]\n","    layers +=[nn.Conv2d(cfg[5], cfg[6], kernel_size = (1))]\n","    layers +=[nn.Conv2d(cfg[6], cfg[7], kernel_size = (3))]\n","\n","    return nn.ModuleList(layers)\n","\n","extras_test = make_extras()\n","for n, p in extras_test.state_dict().items():\n","    print(n)\n","print(extras_test)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ybafLmQllKy5","executionInfo":{"status":"ok","timestamp":1663056714866,"user_tz":-540,"elapsed":29,"user":{"displayName":"bong bong","userId":"17774416216702601338"}},"outputId":"b49ca196-28c7-4b3c-e7a7-71f43d7e372a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["0.weight\n","0.bias\n","1.weight\n","1.bias\n","2.weight\n","2.bias\n","3.weight\n","3.bias\n","4.weight\n","4.bias\n","5.weight\n","5.bias\n","6.weight\n","6.bias\n","7.weight\n","7.bias\n","ModuleList(\n","  (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n","  (1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n","  (2): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))\n","  (3): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n","  (4): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n","  (5): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1))\n","  (6): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n","  (7): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1))\n",")\n"]}]},{"cell_type":"code","source":["def make_loc_conf(num_classes = 21, bbox_aspect_num = [4,6,6,6,4,4]):\n","    loc_layers = []\n","    conf_layers = []\n","    \n","    loc_layers += [nn.Conv2d(512, bbox_aspect_num[0]*4, kernel_size = 3, padding  =1)]\n","    conf_layers += [nn.Conv2d(512, bbox_aspect_num[0]*num_classes, kernel_size = 3, padding  =1)]\n","\n","    loc_layers += [nn.Conv2d(1024, bbox_aspect_num[1]*4, kernel_size = 3, padding = 1)]\n","    conf_layers += [nn.Conv2d(1024, bbox_aspect_num[1]*num_classes, kernel_size = 3, padding = 1)]\n","\n","    loc_layers += [nn.Conv2d(512, bbox_aspect_num[2]*4, kernel_size = 3, padding = 1)]\n","    conf_layers += [nn.Conv2d(512, bbox_aspect_num[2]*num_classes, kernel_size = 3, padding = 1)]\n","    \n","    loc_layers += [nn.Conv2d(256, bbox_aspect_num[3]*4, kernel_size = 3, padding = 1)]\n","    conf_layers += [nn.Conv2d(256, bbox_aspect_num[3]*num_classes, kernel_size = 3, padding = 1)]\n","    \n","    loc_layers += [nn.Conv2d(256, bbox_aspect_num[4]*4, kernel_size = 3, padding = 1)]\n","    conf_layers += [nn.Conv2d(256, bbox_aspect_num[4]*num_classes, kernel_size = 3, padding = 1)]\n","    \n","    loc_layers += [nn.Conv2d(256, bbox_aspect_num[5]*4, kernel_size = 3, padding = 1)]\n","    conf_layers += [nn.Conv2d(256, bbox_aspect_num[5]*num_classes, kernel_size = 3, padding = 1)]\n","    \n","    return nn.ModuleList(loc_layers), nn.ModuleList(conf_layers)\n","    \n","loc_test, conf_test = make_loc_conf()\n","print(loc_test, conf_test, sep='\\n')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ANpGu6cIo2rW","executionInfo":{"status":"ok","timestamp":1663056715626,"user_tz":-540,"elapsed":782,"user":{"displayName":"bong bong","userId":"17774416216702601338"}},"outputId":"b6833e86-233b-4a4f-879f-937964dd19b2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["ModuleList(\n","  (0): Conv2d(512, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","  (1): Conv2d(1024, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","  (2): Conv2d(512, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","  (3): Conv2d(256, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","  (4): Conv2d(256, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","  (5): Conv2d(256, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",")\n","ModuleList(\n","  (0): Conv2d(512, 84, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","  (1): Conv2d(1024, 126, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","  (2): Conv2d(512, 126, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","  (3): Conv2d(256, 126, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","  (4): Conv2d(256, 84, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","  (5): Conv2d(256, 84, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",")\n"]}]},{"cell_type":"code","source":["class L2Norm(nn.Module):\n","    def __init__(self, input_channels = 512, scale = 20):\n","        super(L2Norm, self).__init__()\n","        self.weight = nn.Parameter(torch.Tensor(input_channels))\n","        self.scale = scale\n","        self.reset_parameters()\n","        self.eps = 1e-10\n","\n","    def reset_parameters(self):\n","        nn.init.constant_(self.weight, self.scale)\n","    \n","    def forward(self, x):\n","        norm = x.pow(2).sum(dim=1, keepdim = True).sqrt() + self.eps\n","        x = torch.div(x,norm)\n","        weight = self.weight.unsqueeze(0).unsqueeze(2).unsqueeze(3).expand_as(x)\n","        out = weight * x\n","        return out"],"metadata":{"id":"Jl5jOTwMrJcl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from itertools import product\n","from math import sqrt\n","class DBox():\n","    def __init__(self, cfg):\n","        super(DBox, self).__init__()\n","        self.image_size = cfg['input_size']\n","        self.feature_maps = cfg['feature_maps']\n","        self.steps = cfg['steps']\n","        self.num_priors = len(cfg['feature_maps'])\n","        self.min_sizes = cfg['min_sizes']\n","        self.max_sizes = cfg['max_sizes']\n","        self.aspect_ratios = cfg['aspect_ratios']\n","\n","    def make_dbox_list(self):\n","        mean = []\n","        for k, f in enumerate(self.feature_maps):\n","            for i, j in product(range(f), repeat = 2):\n","                f_k = self.image_size / self.steps[k]\n","                cx = (j + 0.5) / f_k\n","                cy = (i + 0.5) / f_k\n","\n","                s_k = self.min_sizes[k] / self.image_size\n","                mean +=[cx, cy, s_k, s_k]\n","\n","                s_k_prime = sqrt(s_k * (self.max_sizes[k] / self.image_size)) \n","                mean +=[cx, cy, s_k_prime, s_k_prime]\n","\n","                for ar in self.aspect_ratios[k]:\n","                    mean +=[cx, cy, s_k * sqrt(ar), s_k / sqrt(ar)]\n","                    mean +=[cx, cy, s_k / sqrt(ar), s_k * sqrt(ar)]\n","        output = torch.Tensor(mean).view(-1,4)\n","        output.clamp_(max = 1, min = 0)\n","        return output\n","\n","\n","\n","# dbox = DBox(ssd_cfg)\n","# dbox_list = dbox.make_dbox_list()\n","\n","# pd.DataFrame(dbox_list.numpy())\n"],"metadata":{"id":"KrxmIW4QyJEf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def decimate(tensor, m):\n","    \"\"\"\n","    Decimate a tensor by a factor 'm', i.e. downsample by keeping every 'm'th value.\n","    This is used when we convert FC layers to equivalent Convolutional layers, BUT of a smaller size.\n","    :param tensor: tensor to be decimated\n","    :param m: list of decimation factors for each dimension of the tensor; None if not to be decimated along a dimension\n","    :return: decimated tensor\n","    \"\"\"\n","    assert tensor.dim() == len(m)\n","    for d in range(tensor.dim()):\n","        if m[d] is not None:\n","            tensor = tensor.index_select(dim=d,\n","                                         index=torch.arange(start=0, end=tensor.size(d), step=m[d]).long())\n","\n","    return tensor"],"metadata":{"id":"MOrLNmBlIT4a"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class SSD(nn.Module):\n","    def __init__(self, phase, cfg):\n","        super(SSD, self).__init__()\n","        self.phase = phase \n","        self.num_classes = cfg['num_classes']\n","\n","        self.vgg = make_vgg()\n","        self.load_pretrained_layers()\n","\n","        self.extras = make_extras()\n","        self.L2Norm = L2Norm()\n","        self.loc, self.conf = make_loc_conf(cfg['num_classes'], cfg['bbox_aspect_num'])\n","        dbox = DBox(cfg)\n","        self.dbox_list = dbox.make_dbox_list()\n","\n","        if phase == 'inference':\n","            self.detect = Detect()\n","\n","    def forward(self, x):\n","        sources = list()\n","        loc = list()\n","        conf = list()\n","        for k in range(23):\n","            x = self.vgg[k](x)\n","        source1 = self.L2Norm(x)\n","        sources.append(source1)\n","\n","        for k in range(23, len(self.vgg)):\n","            x = self.vgg[k](x)\n","        sources.append(x)\n","\n","        for k, v in enumerate(self.extras):\n","            x = nn.functional.relu(v(x), inplace=True)\n","            if k %2 == 1:\n","                sources.append(x)\n","        \n","        for (x,l,c) in zip(sources, self.loc, self.conf):\n","            loc.append(l(x).permute(0,2,3,1).contiguous())\n","            conf.append(c(x).permute(0,2,3,1).contiguous())\n","\n","        loc = torch.cat([o.view(o.size(0), -1) for o in loc] ,1)\n","        conf = torch.cat([o.view(o.size(0), -1) for o in conf] ,1)\n","\n","        # pdb.set_trace()\n","\n","        loc = loc.view(loc.size(0), -1,4)\n","        conf = conf.view(conf.size(0), -1, self.num_classes)\n","\n","        output = (loc, conf, self.dbox_list)\n","\n","        if self.phase == 'inference':\n","            return self.detect(output[0], output[1], output[2])\n","        else :\n","            return output\n","    def load_pretrained_layers(self):\n","        \"\"\"\n","        As in the paper, we use a VGG-16 pretrained on the ImageNet task as the base network.\n","        There's one available in PyTorch, see https://pytorch.org/docs/stable/torchvision/models.html#torchvision.models.vgg16\n","        We copy these parameters into our network. It's straightforward for conv1 to conv5.\n","        However, the original VGG-16 does not contain the conv6 and con7 layers.\n","        Therefore, we convert fc6 and fc7 into convolutional layers, and subsample by decimation. See 'decimate' in utils.py.\n","        \"\"\"\n","        # Current state of base\n","        state_dict = self.state_dict()\n","        param_names = list(state_dict.keys())\n","\n","        # Pretrained VGG base\n","        pretrained_state_dict = torchvision.models.vgg16(pretrained=True).state_dict()\n","        pretrained_param_names = list(pretrained_state_dict.keys())\n","\n","        # Transfer conv. parameters from pretrained model to current model\n","        for i, param in enumerate(param_names[:-4]):  # excluding conv6 and conv7 parameters\n","            state_dict[param] = pretrained_state_dict[pretrained_param_names[i]]\n","\n","        # Convert fc6, fc7 to convolutional layers, and subsample (by decimation) to sizes of conv6 and conv7\n","        # fc6\n","        conv_fc6_weight = pretrained_state_dict['classifier.0.weight'].view(4096, 512, 7, 7)  # (4096, 512, 7, 7)\n","        conv_fc6_bias = pretrained_state_dict['classifier.0.bias']  # (4096)\n","        state_dict['vgg.31.weight'] = decimate(conv_fc6_weight, m=[4, None, 3, 3])  # (1024, 512, 3, 3)\n","        state_dict['vgg.31.bias'] = decimate(conv_fc6_bias, m=[4])  # (1024)\n","        # # fc7\n","        conv_fc7_weight = pretrained_state_dict['classifier.3.weight'].view(4096, 4096, 1, 1)  # (4096, 4096, 1, 1)\n","        conv_fc7_bias = pretrained_state_dict['classifier.3.bias']  # (4096)\n","        state_dict['vgg.33.weight'] = decimate(conv_fc7_weight, m=[4, 4, None, None])  # (1024, 1024, 1, 1)\n","        state_dict['vgg.33.bias'] = decimate(conv_fc7_bias, m=[4])  # (1024)\n","\n","        # Note: an FC layer of size (K) operating on a flattened version (C*H*W) of a 2D image of size (C, H, W)...\n","        # ...is equivalent to a convolutional layer with kernel size (H, W), input channels C, output channels K...\n","        # ...operating on the 2D image of size (C, H, W) without padding\n","\n","        self.load_state_dict(state_dict)\n","\n","        print(\"\\nLoaded base model.\\n\")\n","# ssd_test = SSD(phase = 'train', cfg = ssd_cfg)\n","# print(ssd_test)"],"metadata":{"id":"jH9KmjwF1xbb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def decode(loc, dbox_list):\n","    boxes = torch.cat((\n","        dbox_list[:,:2] + loc[:, :2] * 0.1 * dbox_list[:, 2:],\n","        dbox_list[:,2:] * torch.exp(loc[:,2:] *0.2)), dim = 1)\n","    \n","    boxes[:,:2] -= boxes[:,2:] /2\n","    boxes[:,2:] += boxes[:,:2] \n","    return boxes"],"metadata":{"id":"Ba_iKDbGbBEo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","def nm_suppression(boxes, scores, overlap = .45, top_k = 200):\n","    count = 0\n","    keep = scores.new(scores.size(0).zero_().long())\n","    x1= boxes[:,0]\n","    y1= boxes[:,1]\n","    x2= boxes[:,2]\n","    y2= boxes[:,3]\n","    area = torch.mul(x2-x1, y2,y1)\n","\n","    tmp_x1 = boxes.new_tensor()\n","    tmp_y1 = boxes.new_tensor()\n","    tmp_x2 = boxes.new_tensor()\n","    tmp_y2 = boxes.new_tensor()\n","    tmp_w = boxes.new_tensor()\n","    tmp_h = boxes.new_tensor()\n","\n","    # pdb.set_trace()\n","\n","    v, idx = scores.sort(0)\n","    idx = idx[-top_k:]\n","\n","    while idx.numel() > 0:\n","        i = idx[-1]\n","        keep[count] = i\n","        count+=1\n","\n","        if idx.size(0) ==1:\n","            break\n","        \n","        idx = idx[:-1]\n","        torch.index_select(x1,0,idx,out = tmp_x1)\n","        torch.index_select(y1,0,idx,out = tmp_y1)\n","        torch.index_select(x2,0,idx,out = tmp_x2)\n","        torch.index_select(y2,0,idx,out = tmp_y2)\n","\n","        tmp_x1 = torch.clamp(tmp_x1, min = x1[i])\n","        tmp_y1 = torch.clamp(tmp_y1, min = y1[i])\n","        tmp_x2 = torch.clamp(tmp_x2, max = x2[i])\n","        tmp_y2 = torch.clamp(tmp_y2, max = y2[i])\n","\n","        tmp_w.resize_as_(tmp_x2)\n","        tmp_h.resize_as_(tmp_y2)\n","\n","        tmp_w = tmp_x2 - tmp_x1\n","        tmp_h = tmp_y2 - tmp_y1\n","\n","        tmp_w = torch.clamp(tmp_w, min = 0.0)\n","        tmp_h = torch.clamp(tmp_h, min = 0.0)\n","\n","        inter = tmp_w * tmp_h\n","\n","        rem_areas = torch.index_select(area , 0, idx)\n","        union = (rem_areas - inter) + area[i]\n","        IoU = inter / union\n","\n","        idx = idx[IoU.le(overlap)]\n","\n","        # pdb.set_trace()\n","\n","    return keep, count"],"metadata":{"id":"GkMZ8Sh3dApE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class Detect(Function):\n","    def __init__(self, conf_thresh = 0.01, top_k = 200, nms_thresh = 0.45):\n","        self.softmax = nn.Softmax(dim = -1)\n","        self.conf_thresh = conf_thresh\n","        self.top_k = top_k\n","        self.nms_thresh = nms_thresh\n","\n","    def forward(self, loc_data, conf_data, dbox_list):\n","        num_batch = loc_data.size(0)\n","        num_dbox = loc_data.size(1)\n","        num_classes = loc_data.size(2)\n","        \n","        conf_data = self.softmax(conf_data)\n","        output = torch.zeros(num_batch, num_classes, self.top_k, 5)\n","        conf_preds = conf_data.transpose(2,1)\n","\n","        for i in range(num_batch):\n","            decoded_boxes = decode[loc_data[i], dbox_list]\n","            conf_scores = conf_preds[i].clone()\n","\n","            for cl in range(1, num_classes):\n","                c_mask = conf_scores[cl].gt(self.conf_thresh)\n","                scores = conf_scores[cl][c_mask]\n","\n","                if scores.nelement() == 0:\n","                    continue\n","                l_mask = c_mask.unsqueeze(1).expand_as(decoded_boxes)\n","                boxes = decoded_boxes[l_mask].view(-1,4)\n","\n","                ids, count = nm_suppression(boxes, scores, self.nms_thresh, self.top_k)\n","                output[i, cl, :count] = torch.cat((scores[ids][:count].unsqueeze(1), boxes[ids[:count]]),1)\n","        return output"],"metadata":{"id":"raTnilhJgj-t"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class MultiBoxLoss(nn.Module):\n","    def __init__(self, jaccard_thresh = .5, neg_pos = 3,  device = 'cpu'):\n","        super(MultiBoxLoss, self).__init__()\n","        self.jaccard_thresh = jaccard_thresh\n","        self.negpos_ratio = neg_pos\n","        self.device = device\n","    \n","    def forward(self, predictions, targets):\n","        \"\"\"\n","        predictions : (loc = torch.Size([num_batch, 8732,4]),\n","                       conf = torch.Size([num_batch, 8732, 21]),\n","                       dbox = torch.Size([8732, 4])\n","\n","        targets : [xmin, ymin, xmax, ymax, label_ind]\n","        \"\"\"\n","        loc_data , conf_data, dbox_list = predictions\n","        num_batch = loc_data.size(0)\n","        num_dbox = loc_data.size(1)\n","        num_classes = conf_data.size(2)\n","        \n","        # pdb.set_trace()\n","\n","\n","        conf_t_label = torch.LongTensor(num_batch, num_dbox).to(self.device)\n","        loc_t = torch.Tensor(num_batch, num_dbox,4).to(self.device)\n","\n","        for idx in range(num_batch):\n","            truths = targets[idx][:,:-1].to(self.device)\n","            labels = targets[idx][:,-1].to(self.device)\n","            dbox = dbox_list.to(self.device)\n","            variance = [0.1,0.2]\n","\n","            match(self.jaccard_thresh, truths, dbox, variance, labels, loc_t, conf_t_label, idx)\n","        # match(self.jaccrad_trhes, truths, priors, variances, labels, loc_t, conf_t, idx)\n","\n","        pos_mask = conf_t_label >0\n","        pos_idx = pos_mask.unsqueeze(pos_mask.dim()).expand_as(loc_data)\n","        \n","        loc_p = loc_data[pos_idx].view(-1,4)\n","        loc_t = loc_t[pos_idx].view(-1,4)\n","        \n","        \n","        \n","        loss_l = nn.functional.smooth_l1_loss(loc_p, loc_t, reduction = 'sum')\n","\n","        batch_conf = conf_data.view(-1, num_classes)\n","\n","        \"\"\"\n","        \"\"\"\n","        # pdb.set_trace()\n","        \n","        \n","        loss_c = nn.functional.cross_entropy(batch_conf, conf_t_label.view(-1), reduction = 'none')\n","        num_pos = pos_mask.long().sum(1, keepdim= True)\n","\n","        loss_c = loss_c.view(num_batch, -1)\n","        loss_c[pos_mask] = 0\n","\n","        _, loss_idx = loss_c.sort(1, descending = True)\n","        _, idx_rank = loss_idx.sort(1)\n","\n","        num_neg = torch.clamp(num_pos * self.negpos_ratio, max = num_dbox)\n","        neg_mask = idx_rank < (num_neg).expand_as(idx_rank)\n","\n","        pos_idx_mask = pos_mask.unsqueeze(2).expand_as(conf_data)\n","        neg_idx_mask = neg_mask.unsqueeze(2).expand_as(conf_data)\n","\n","        conf_hnm = conf_data[(pos_idx_mask + neg_idx_mask).gt(0)].view(-1, num_classes)\n","        conf_t_label_hnm = conf_t_label[(pos_mask + neg_mask).gt(0)]\n","\n","        loss_c = nn.functional.cross_entropy(conf_hnm, conf_t_label_hnm, reduction = 'sum')\n","\n","        N = num_pos.sum()\n","        loss_l /= N\n","        loss_c /= N\n","\n","        return loss_l, loss_c"],"metadata":{"id":"QrPzbu6_vZTf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["rootpath = '/content/VOCdevkit/VOC2012/'\n","train_img_list, train_anno_list, val_img_list, val_anno_list = make_datapath_list(rootpath)\n","\n","voc_classes = ['aeroplane','bicycle','bird','boat',\n","               'bottle','bus','car','cat',\n","               'chair','cow','diningtable','dog',\n","               'horse','motorbike','person','pottedplant',\n","               'sheep','sofa','train','tvmonitor']\n","\n","color_mean = (104,117,123)\n","input_size = 300\n","\n","train_dataset = VOCDataset(train_img_list, train_anno_list, phase = 'train', transform = DataTransform(input_size, color_mean),\n","                           transform_anno  = Anno_xml2list(voc_classes))\n","val_dataset = VOCDataset(val_img_list, val_anno_list, phase = 'val', transform = DataTransform(input_size, color_mean),\n","                           transform_anno  = Anno_xml2list(voc_classes))\n","\n","batch_size = 4\n","\n","train_dataloader = data.DataLoader(train_dataset, batch_size = batch_size, shuffle = True, collate_fn = od_collate_fn)\n","val_dataloader = data.DataLoader(val_dataset, batch_size = batch_size, shuffle = False, collate_fn = od_collate_fn)\n","\n","dataloaders_dict = {'train' : train_dataloader, 'val' : val_dataloader}"],"metadata":{"id":"6mha1-Hz2VuQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["ssd_cfg = {\n","    'num_classes':21,\n","    'input_size': 300,\n","    'bbox_aspect_num' : [4,6,6,6,4,4],\n","    'feature_maps' : [38,19,10,5,3,1],\n","    'steps': [8,16,32,64,100,300],\n","    'min_sizes': [30,60,111,162,213,264],\n","    'max_sizes': [60,111,162,213,264, 315],\n","    'aspect_ratios' : [[2],[2,3],[2,3],[2,3],[2],[3]] \n","    }\n","\n","net = SSD(phase = 'train', cfg = ssd_cfg)\n","\n","# vgg_weights = torch.load(torchvision.models.vgg16(pretrained=True).state_dict())\n","# net.vgg.load_state_dict(vgg_weights)\n","\n","def weights_init(m):\n","    if isinstance(m, nn.Conv2d):\n","        nn.init.kaiming_normal_(m.weight.data)\n","        if m.bias is not None :\n","            nn.init.constant_(m.bias, 0.0)\n","net.extras.apply(weights_init)\n","net.loc.apply(weights_init)\n","net.conf.apply(weights_init)\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":214,"referenced_widgets":["d356c9ec5a2640288e686584349e90d7","8f72c01021734dd5837dc486a358a5c5","fbbd5e378a774e78997ce58568003d5a","3ef4bb1bbd3f4c2589a5572ecf8152ad","cb14e71b1b394836ba9166244feaa9c9","761cd9d638334f91a540109aa5a5d2f5","472039593c914997aeae18c3c9c0b103","f92f4473415c49f181a12d6231244741","bcd8f021a9894a0bafc3ee8f424800c6","8a78d99edde34327b9409e17a00febd1","b2f8565e9ea84c8e87aed071a24a8da3"]},"id":"NN52GfpU5ZPt","executionInfo":{"status":"ok","timestamp":1663056726500,"user_tz":-540,"elapsed":10884,"user":{"displayName":"bong bong","userId":"17774416216702601338"}},"outputId":"14fd7aac-ed00-4e73-f5e9-f532f2517e29"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torchvision/models/_utils.py:209: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n","  f\"The parameter '{pretrained_param}' is deprecated since 0.13 and will be removed in 0.15, \"\n","/usr/local/lib/python3.7/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n","Downloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to /root/.cache/torch/hub/checkpoints/vgg16-397923af.pth\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0.00/528M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d356c9ec5a2640288e686584349e90d7"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n","Loaded base model.\n","\n"]}]},{"cell_type":"code","source":["criterion = MultiBoxLoss(jaccard_thresh = 0.5, neg_pos = 3, device = device)\n","optimizer = torch.optim.AdamW(net.parameters(), lr = 1e-3, weight_decay = 5e-4)"],"metadata":{"id":"FQwivSRYT7Y9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import time\n","import tqdm\n","def train_model(net, dataloaders_dict, criterion, optimizer, num_epochs):\n","    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","    print(f'{device}')\n","\n","    net.to(device)\n","    torch.backends.cudnn.benchmark = True\n","\n","    iteration = 1\n","    epoch_train_loss = 0.0\n","    epoch_val_loss = 0.0\n","    logs = []\n","\n","    for epoch in range(num_epochs +1):\n","        t_epoch_start = time.time()\n","        t_iter_start = time.time()\n","\n","        print('--------------------------------------')\n","        print(f'Epoch {epoch+1}/{num_epochs}')\n","        print('--------------------------------------')\n","\n","        for phase in ['val']:\n","            if phase == 'train':\n","                net.train()\n","                print('train mode')\n","            else:\n","                if((epoch +1) % 10 == 0):\n","                    net.eval()\n","                    print('val mode')\n","                else :\n","                    continue\n","\n","            for images, targets in tqdm.tqdm(dataloaders_dict[phase]):\n","                \n","                \n","                # pdb.set_trace()\n","                \n","                \n","                images = images.to(device)\n","                targets = [ann.to(device) for ann in targets]\n","                optimizer.zero_grad()\n","\n","                with torch.set_grad_enabled(phase == 'train'):\n","                    outputs = net(images)\n","                    loss_l, loss_c = criterion(outputs, targets)\n","                    loss = loss_l + loss_c\n","\n","                    if phase == 'train':\n","                        loss.backward()\n","                        nn.utils.clip_grad_value_(net.parameters(), clip_value = 2.0)\n","                        optimizer.step()\n","\n","                        if (iteration % 10 == 0 ):\n","                            t_iter_finish = time.time() \n","                            duration = t_iter_finish - t_iter_start\n","                            print(f'반복 {iteration} || Loss : {loss.item():4f} || 10iter : {duration :.4f}')\n","\n","                            t_iter_start = time.time()\n","                        \n","                        epoch_train_loss += loss.item()\n","                        iteration +=1\n","                    \n","                    else:\n","                        epoch_val_loss += loss.item()\n","\n","        t_epoch_finish = time.time()\n","        print('--------------------------------------')\n","        print(f'Epoch{epoch +1} || Epoch train loss : {epoch_train_loss} Epoch val loss {epoch_val_loss}')\n","        print('--------------------------------------')    \n","        print(f'timer : {t_epoch_finish - t_epoch_start : .4f}')\n","\n","        log_epoch = {'epoch' : epoch+1,\n","                     'train_loss' : epoch_train_loss,\n","                     'val_loss' : epoch_val_loss}\n","        logs.append(log_epoch)\n","        df = pd.DataFrame(logs)\n","        df.to_csv('log_output.csv')\n","\n","        epoch_train_loss = 0.0\n","        epoch_val_loss = 0.0\n","\n","        if ((epoch+1) %10 == 0):\n","            torch.save(net.state_dict(), 'weight/ssd300_' + str(epoch+1)+'.pth')"],"metadata":{"id":"iIR6bb_8UL4C"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# torch.cuda.empty_cache()\n","os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n","os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""],"metadata":{"id":"2oqFvzVNajhm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["num_epochs = 50\n","train_model(net, dataloaders_dict, criterion, optimizer, num_epochs = num_epochs)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"jPNLiHWqYDJ7","executionInfo":{"status":"error","timestamp":1663060642449,"user_tz":-540,"elapsed":137578,"user":{"displayName":"bong bong","userId":"17774416216702601338"}},"outputId":"878ee760-e478-44d1-bbd9-41859d2ae7b9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["cuda\n","--------------------------------------\n","Epoch 1/50\n","--------------------------------------\n","--------------------------------------\n","Epoch1 || Epoch train loss : 0.0 Epoch val loss 0.0\n","--------------------------------------\n","timer :  0.0001\n","--------------------------------------\n","Epoch 2/50\n","--------------------------------------\n","--------------------------------------\n","Epoch2 || Epoch train loss : 0.0 Epoch val loss 0.0\n","--------------------------------------\n","timer :  0.0007\n","--------------------------------------\n","Epoch 3/50\n","--------------------------------------\n","--------------------------------------\n","Epoch3 || Epoch train loss : 0.0 Epoch val loss 0.0\n","--------------------------------------\n","timer :  0.0001\n","--------------------------------------\n","Epoch 4/50\n","--------------------------------------\n","--------------------------------------\n","Epoch4 || Epoch train loss : 0.0 Epoch val loss 0.0\n","--------------------------------------\n","timer :  0.0001\n","--------------------------------------\n","Epoch 5/50\n","--------------------------------------\n","--------------------------------------\n","Epoch5 || Epoch train loss : 0.0 Epoch val loss 0.0\n","--------------------------------------\n","timer :  0.0006\n","--------------------------------------\n","Epoch 6/50\n","--------------------------------------\n","--------------------------------------\n","Epoch6 || Epoch train loss : 0.0 Epoch val loss 0.0\n","--------------------------------------\n","timer :  0.0006\n","--------------------------------------\n","Epoch 7/50\n","--------------------------------------\n","--------------------------------------\n","Epoch7 || Epoch train loss : 0.0 Epoch val loss 0.0\n","--------------------------------------\n","timer :  0.0007\n","--------------------------------------\n","Epoch 8/50\n","--------------------------------------\n","--------------------------------------\n","Epoch8 || Epoch train loss : 0.0 Epoch val loss 0.0\n","--------------------------------------\n","timer :  0.0006\n","--------------------------------------\n","Epoch 9/50\n","--------------------------------------\n","--------------------------------------\n","Epoch9 || Epoch train loss : 0.0 Epoch val loss 0.0\n","--------------------------------------\n","timer :  0.0007\n","--------------------------------------\n","Epoch 10/50\n","--------------------------------------\n","val mode\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1456/1456 [02:17<00:00, 10.56it/s]"]},{"output_type":"stream","name":"stdout","text":["--------------------------------------\n","Epoch10 || Epoch train loss : 0.0 Epoch val loss 10764.336997032166\n","--------------------------------------\n","timer :  137.8881\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"error","ename":"FileNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-33-26192259c68d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mnum_epochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloaders_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-31-ca989fbfa22c>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(net, dataloaders_dict, criterion, optimizer, num_epochs)\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;36m10\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'weight/ssd300_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'.pth'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization)\u001b[0m\n\u001b[1;32m    374\u001b[0m     \u001b[0m_check_dill_version\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpickle_module\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    375\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 376\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    377\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_use_new_zipfile_serialization\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0m_open_zipfile_writer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_is_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    231\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'w'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_open_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'weight/ssd300_10.pth'"]}]}]}