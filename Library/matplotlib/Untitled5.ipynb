{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Untitled5.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyOsKKEvow/7X5oIqVQAF5+x"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"wIMbwAYe_yEX","executionInfo":{"status":"ok","timestamp":1655961581788,"user_tz":-540,"elapsed":21,"user":{"displayName":"bong bong","userId":"17774416216702601338"}}},"outputs":[],"source":["import numpy as np"]},{"cell_type":"code","source":["def preprocess(text) :\n","  text = text.lower()\n","  text = text.replace('.', ' .')\n","  words = text.split(' ')\n","\n","  word_to_id = {}\n","  id_to_word = {}\n","  for word in words :\n","    if word not in word_to_id:\n","      new_id = len(word_to_id)\n","      word_to_id[word] = new_id\n","      id_to_word[new_id] = word\n","\n","  corpus = np.array([word_to_id[w] for w in words])\n","  return corpus, word_to_id, id_to_word\n","\n","\n","def create_co_matrix(corpus, vocab_size, window_size =1):\n","  corpus_size = len(corpus)\n","  co_matrix = np.zeros((vocab_size, vocab_size), dtype=np.float32)\n","\n","  for idx, word_id in enumerate(corpus):\n","    for size in range(1, window_size+1):\n","      left_idx = idx - size\n","      right_idx = idx + size\n","\n","      if left_idx >=0 :\n","        left_word_id = corpus[left_idx]\n","        co_matrix[word_id, left_word_id] +=1\n","      \n","      if right_idx <= vocab_size:\n","        right_word_id = corpus[right_idx]\n","        co_matrix[word_id, right_word_id] += 1\n","\n","  return co_matrix\n","\n","\n","def cos_similarity(x:np.array, y:np.array, eps=1e-8) :\n","  nx = x / np.sqrt(np.sum(x**2) +eps)\n","  ny = y / np.sqrt(np.sum(y**2)+eps)\n","  cos = np.matmul(nx,ny)\n","\n","  return cos\n","\n","def most_similar(query, word_to_id:dict, id_to_word :dict, co_matrix, top=5):\n","  \n","  if query not in word_to_id :\n","    print(f'{query}라는 단어는 없음')\n","    return\n","  \n","  print('\\n[query]' + query)\n","  query_id = word_to_id[query]\n","  query_vec = co_matrix[query_id]\n","\n","  vocab_size = len(word_to_id)\n","  similarity = np.zeros(vocab_size)\n","  for i in range(vocab_size) :\n","    similarity[i]= cos_similarity(query_vec, co_matrix[i])\n","\n","  count = 0\n","  for j in (-1*similarity).argsort():\n","    if id_to_word[j] == query:\n","      continue\n","    print('%s와의 유사도: %s' % (id_to_word[j], similarity[j]))\n","\n","    count+= 1\n","\n","    if count>=top:\n","      return\n","    \n","\n","def ppmi(C :np.array, verbose = False, eps =1e-8):\n","  M = np.zeros_like(C, dtype = np.float32)\n","  N = np.sum(C)\n","  S = np.sum(C,axis = 0)\n","\n","  total = C.shape[0] * C.shape[1]\n","  cnt = 0\n","\n","  for i in range(C.shape[0]) :\n","    for j in range(C.shape[1]):\n","      pmi = np.log2(C[i,j] * N / (S[i] * S[j]) +eps)\n","      M[i,j] = max(0, pmi)\n","\n","      if verbose :\n","        cnt+= 1\n","        if cnt % (total // 100) ==0:\n","          print(f'{100 *cnt/total : .1f} %완료')\n","  return M\n","\n","def create_contexts_target(corpus : list, window_size =1):\n","\n","  target = corpus[window_size : -window_size]\n","  contexts = []\n","\n","  for idx in range(window_size , len(corpus) - window_size):\n","    cs = []\n","    for t in range(-window_size , window_size +1):\n","      if t ==0 :\n","        continue\n","      cs.append(corpus[idx +t])\n","    contexts.append(cs)\n","  \n","  return np.array(contexts), np.array(target)\n","  \n","\n","def convert_one_hot(corpus:np.array, vocab_size):\n","  N = corpus.shape[0]\n","  \n","  if corpus.ndim ==1:\n","    one_hot = np.zeros((N,vocab_size), dtype=np.int32)\n","    for idx, word_id in enumerate(corpus):\n","      one_hot[idx, word_id] =1\n","\n","  if corpus.ndim ==2:\n","    C = corpus.shape[1]\n","    # one_hot = np.zeros((N,C,vocab_size), dtype=np.int32)\n","    one_hot = np.zeros((N, C, vocab_size), dtype=np.int32)\n","    for idx_0, word_ids in enumerate(corpus):\n","      for idx_1, word_id in enumerate(word_ids):\n","        one_hot[idx_0, idx_1, word_id] = 1\n","\n","  return one_hot\n","\n","\n","\n"],"metadata":{"id":"f3gU9AamAE42","executionInfo":{"status":"ok","timestamp":1655969946850,"user_tz":-540,"elapsed":409,"user":{"displayName":"bong bong","userId":"17774416216702601338"}}},"execution_count":67,"outputs":[]},{"cell_type":"code","source":["text = 'You say goodbye and I say Hello.'\n","corpus, word_to_id, id_to_word = preprocess(text)\n","# co_matrix = create_co_matrix(corpus = corpus, vocab_size = len(word_to_id), window_size = 1)\n","# query = 'you'\n","# most_similar(query, word_to_id, id_to_word, co_matrix, 5)\n","vocab_size = len(word_to_id)\n","C = create_co_matrix(corpus, vocab_size, window_size =1)\n","W = ppmi(C)\n","np.set_printoptions(precision=3)\n","\n","\n","U, S, V = np.linalg.svd(W)\n","# print(U)\n","# print(W)\n","dim_reduce_W = U[:,:2]\n","# print(dim_reduce_W)\n","# print(cs)\n","corpus, target = create_contexts_target(corpus, window_size=1)\n","\n","vocab_size = len(word_to_id)\n","one_hot_corpus = convert_one_hot(corpus, vocab_size)\n","one_hot_target = convert_one_hot(target, vocab_size)\n","\n","print('One-hot corpus shape:', one_hot_corpus.shape)\n","print(one_hot_corpus)\n","print()\n","\n","print('One-hot target shape:', one_hot_target.shape)\n","print(one_hot_target)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jMAKWLjzbY-l","executionInfo":{"status":"ok","timestamp":1655969951438,"user_tz":-540,"elapsed":689,"user":{"displayName":"bong bong","userId":"17774416216702601338"}},"outputId":"d16e5e93-c1a5-4a92-f5d1-f37743c4bb03"},"execution_count":68,"outputs":[{"output_type":"stream","name":"stdout","text":["One-hot corpus shape: (6, 2, 7)\n","[[[1 0 0 0 0 0 0]\n","  [0 0 1 0 0 0 0]]\n","\n"," [[0 1 0 0 0 0 0]\n","  [0 0 0 1 0 0 0]]\n","\n"," [[0 0 1 0 0 0 0]\n","  [0 0 0 0 1 0 0]]\n","\n"," [[0 0 0 1 0 0 0]\n","  [0 1 0 0 0 0 0]]\n","\n"," [[0 0 0 0 1 0 0]\n","  [0 0 0 0 0 1 0]]\n","\n"," [[0 1 0 0 0 0 0]\n","  [0 0 0 0 0 0 1]]]\n","\n","One-hot target shape: (6, 7)\n","[[0 1 0 0 0 0 0]\n"," [0 0 1 0 0 0 0]\n"," [0 0 0 1 0 0 0]\n"," [0 0 0 0 1 0 0]\n"," [0 1 0 0 0 0 0]\n"," [0 0 0 0 0 1 0]]\n"]}]},{"cell_type":"code","source":["import tensorflow.keras as tf\n","\n","class SimpleCBOW :\n","  def __init__(self, vocab_size, hidden_size):\n","    V, H = vocab_size, hidden_size\n","\n","    W_in = 0.01* np.random.randn(V,H).astype('f')\n","    W_out = 0.01* np.random.randn(H,V).astype('f')\n","    \n","    self.in_layer1 = np.matnul(W_in)\n","    self.in_layer2 = np.matnul(W_in)\n","    self.out_layer = np.matnul(W_out)\n","    self.loss_layer = tf.keras.softmax(W_in)\n","\n","    self.params, self.grads= [], []\n","    layers = [self.in_layer1, self.in_layer2, self.out_layer1, self.loss_layer]\n","    for layer in layers:\n","      self.params += layer.params\n","      self.grads += layer.grads\n","    \n","    self.word_vecs = W_in\n","        \n","  def foward(self, contexts, target):\n","    h0 = self.in_layer1.forward(contexts[:,0,:])\n","    h1 = self.in_layer1.forward(contexts[:,1,:])\n","    h = (h0 + h1) *.5\n","    score = self.out_layer.forward(h)\n","    loss = self.loss_layer.forward(score, target)\n","\n","    return loss\n","\n","  def backward(self, dout=1):\n","    d_score = self.loss_layer.backward(dout)\n","    d_h = self.out_layer.backward(d_score)\n","    d_h *= .5\n","    self.in_layer1.backward(d_h)\n","    self.in_layer2.backward(d_h)\n","\n","    return None\n","\n"],"metadata":{"id":"9KHzzCfhj9mC","executionInfo":{"status":"ok","timestamp":1655971586605,"user_tz":-540,"elapsed":406,"user":{"displayName":"bong bong","userId":"17774416216702601338"}}},"execution_count":70,"outputs":[]},{"cell_type":"code","source":["def backward(self, dh_next, dc_next):\n","        Wx, Wh, b = self.params\n","        x, h_prev, c_prev, f, g, i, o, c_next = self.cache\n","        #===============\n","        # 게이트 역전파 수행\n","        #===============\n","        tanh_c_next = np.tanh(c_next)\n","        \n","        ds = dh_next * o * (1 - tanh_c_next**2) + dc_next\n","        \n","        dc_prev = ds * f  # 이전 기억 셀의 기울기\n","        \n","        # output 게이트\n","        do = dh_next * tanh_c_next\n","        do *= o * (1 - o)\n","        # input 게이트\n","        di = ds * g\n","        di *= i * (1 - i)\n","        # 새로운 기억 셀(main 게이트)\n","        dg = ds * i\n","        dg *= (1 - g**2)\n","        # forget 게이트\n","        df = ds * c_prev\n","        df *= f * (1 - f)\n","        \n","        # 4개 게이트 기울기 가로로 결합, horizontal stack\n","        dA = np.hstack((df, dg, di, do))\n","        \n","        #=================================\n","        # Affine 변환(행렬 곱)에 대한 역전파 수행\n","        #=================================\n","        # 파라미터 기울기 계산\n","        dWx = np.matmul(x.T, dA)\n","        dWh = np.matmul(h_prev.T, dA)\n","        db = dA.sum(axis=0)\n","        \n","        self.grads[0][...] = dWx\n","        self.grads[1][...] = dWh\n","        self.grads[2][...] = db\n","        \n","        # 입력, 은닉상태 벡터 기울기 계싼\n","        dx = np.matmul(dA, Wx.T)\n","        dh_prev = np.matmul(dA, Wh.T)\n","        \n","        return dx, dh_prev, dc_prev"],"metadata":{"id":"XoiOs7w6VFP7"},"execution_count":null,"outputs":[]}]}